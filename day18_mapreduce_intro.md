# ðŸ“˜ Day 18 â€“ MapReduce & Distributed Processing Introduction

## ðŸ”· What is MapReduce?
MapReduce is a programming model used to process large data sets with a distributed algorithm on a cluster.

- **Map Function**: Takes input and emits intermediate key-value pairs
- **Reduce Function**: Aggregates intermediate data and produces output

## ðŸ”· Why Use MapReduce?
- Handles large volumes of data by distributing processing
- Scalable across clusters
- Fault tolerant
- Part of core Hadoop processing engine

---

## ðŸ”· MapReduce Workflow
1. **Input Split** â€“ Data is divided into splits
2. **Map Phase** â€“ Each split is processed into key-value pairs
3. **Shuffle & Sort** â€“ Grouped by key across nodes
4. **Reduce Phase** â€“ Aggregates results
5. **Final Output** â€“ Stored back in HDFS

---

## ðŸ”· Cluster Interaction
- Mappers run in parallel across nodes
- Data movement handled automatically via HDFS
- Resource management by YARN or JobTracker (older)
