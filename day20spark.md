# üöÄ Day 20: Apache Spark ‚Äì Big Data Processing Framework

Today marks the beginning of my Apache Spark journey, diving into the power of distributed processing for big data. Spark is not just fast‚Äîit's built to scale, process, and transform massive volumes of data with ease.

---

## üîç Topics Covered

1. **What is Apache Spark?**
   - Open-source unified analytics engine for big data processing
   - Supports batch, streaming, ML, and graph workloads

2. **Why Spark over Hadoop?**
   | Feature         | Spark                          | Hadoop MapReduce            |
   |-----------------|--------------------------------|-----------------------------|
   | Speed           | In-memory computation          | Disk-based processing       |
   | API             | Rich high-level APIs (Scala, Python, Java, R) | Low-level Java APIs |
   | Use Cases       | ML, Streaming, ETL             | Mainly Batch processing     |

3. **Core Spark Components**
   - Spark Core (RDDs)
   - Spark SQL
   - Spark Streaming
   - MLlib
   - GraphX

4. **Advantages of Spark**
   - Speed (100x faster than MapReduce)
   - Easy to use APIs
   - General-purpose engine
   - Fault tolerance with DAG lineage
   - Lazy evaluation

---

## üß† Common Interview Questions

- Why Spark is faster than Hadoop?
- What are RDDs and why are they immutable?
- Difference between Spark SQL and Hive?
- How does Spark handle fault tolerance?
- When to use DataFrame vs RDD?

---
