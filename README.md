# üöÄ My Big Data Learning Journey

Welcome to my personal learning journey into the world of **Big Data**, where I'm diving deep into **SQL, Python, GCP, Azure**, and various **data engineering concepts**. I'm documenting everything I learn, practice, and build ‚Äî one day at a time.

---

## üóìÔ∏è Daily Learning Progress

### ‚úÖ **Day 1 ‚Äì Introduction to Big Data Engineering**
- Understood the role and responsibilities of a Big Data Engineer.
- Explored real-world use cases.
- Set up my learning goals and plan.

### ‚úÖ **Day 2 ‚Äì SQL Basics**
- Created databases and tables.
- Inserted, updated, and deleted records.
- Practiced DML operations and table alterations.

### ‚úÖ **Day 3 ‚Äì Constraints and Indexes**
- Learned about Primary Key, Foreign Key, Unique, Not Null, and Check constraints.
- Explored Indexing and Candidate Keys.

### ‚úÖ **Day 4 ‚Äì SQL Joins**
- Practiced INNER, LEFT, RIGHT, FULL OUTER, and CROSS JOINs.
- Understood data relationships and merging tables.

### ‚úÖ **Day 5 ‚Äì Subqueries**
- Worked with single-row, multi-row, nested, and correlated subqueries.
- Learned how subqueries can solve complex logic within SQL.

### ‚úÖ **Day 6 ‚Äì Common Table Expressions (CTEs)**
- Simplified queries using CTEs.
- Explored recursive CTEs for hierarchical data.

### ‚úÖ **Day 7 ‚Äì Window Functions**
- Practiced ROW_NUMBER(), RANK(), DENSE_RANK().
- Used PARTITION BY and ORDER BY within the OVER() clause.

### ‚úÖ **Day 8 ‚Äì Python Revision Begins**
- Reviewed Python basics: syntax, variables, data types, loops, conditionals, functions, and collections (lists, tuples, sets, dictionaries).
- Code and notes uploaded in this repository.

‚úÖ **Day 9 ‚Äì Python Functional Programming & File Handling**
- Learned Lambda functions, `map()`, and `filter()` for cleaner and faster operations.
- Explored Python modules, the standard library, and importing techniques.
- Practiced file operations: reading, writing, and working with file paths.
- Understood and implemented exception handling to build error-resilient programs.
- Code and examples added to the repository.

‚úÖ **Day 10 ‚Äì Object-Oriented Programming in Python**
- Revised and practiced key OOP concepts to improve code structure and reusability.
- Topics covered:
  - **Classes & Objects** ‚Äì The foundation of OOP in Python.
  - **Inheritance** ‚Äì Reusing code through parent-child class structures.
  - **Polymorphism** ‚Äì Allowing methods to behave differently depending on the object.
  - **Encapsulation** ‚Äì Keeping data secure and private within objects.
  - **Abstraction** ‚Äì Hiding internal implementation and exposing only necessary parts.

‚úÖ **Day 11 ‚Äì Magic Methods, Iterators & Decorators**
- Explored advanced Python concepts to write more Pythonic, powerful, and reusable code.
- Topics covered:
  - Magic Methods (`__str__`, `__repr__`, `__add__`, etc.)
  - Custom Exceptions
  - Operator Overloading
  - Iterators & Iterables
  - Generators using `yield`
  - Function Decorators

‚úÖ **Day 12 ‚Äì Python Libraries for Data Handling & Integration**
- Focused on real-world data manipulation and connecting Python to databases.
- Topics covered:
  - NumPy ‚Äì Arrays and numerical operations
  - Pandas ‚Äì DataFrames, cleaning, filtering, and reshaping data
  - Data Cleaning ‚Äì Handling nulls, duplicates, formatting
  - Logging ‚Äì Structured logging and debugging
  - SQLite with Python ‚Äì Creating DBs, inserting records, querying with `sqlite3`

‚úÖ **Day 13 ‚Äì Introduction to Big Data Concepts**
- Learned the foundational concepts of Big Data and modern system architecture.
- Topics covered:
  - What is Big Data ‚Äì Real-world example
  - 5 V's of Big Data ‚Äì Volume, Velocity, Variety, Veracity, Value
  - Distributed Systems ‚Äì Why and how we scale
  - Designing a Good Big Data System ‚Äì Best practices
  - On-Premise vs Cloud Infrastructure
  - Database vs Data Warehouse vs Data Lake
  - ETL vs ELT ‚Äì Understanding transformation flow.
  
‚úÖ **Day 14 ‚Äì Hadoop Architecture & Ecosystem**
- Learned the core fundamentals of Hadoop and its role in distributed data processing.
- Topics covered:
  - Hadoop Architecture: GFS, MapReduce
  - Hadoop Properties: Scalability, Fault Tolerance, Distributed Processing, Cost-Effectiveness, Open Source
  - HDFS (Hadoop Distributed File System): File system blocks, replication, rack awareness, metadata handling
  - Hadoop Ecosystem Overview:
    - **Storage**: HDFS, HBase
    - **Processing**: MapReduce, Pig, Hive, Spark
    - **Data Integration**: Flume, Sqoop
    - **Workflow & Coordination**: Oozie, ZooKeeper
    - **ML**: Mahout

‚úÖ **Day 15 ‚Äì GCP Cluster Setup & DataNode Failures**
- Took the first step into cloud platforms by creating a cluster on Google Cloud Platform (GCP).
- Topics covered:
  - GCP setup: Compute Engine, firewall rules, SSH configuration
  - Created a basic Hadoop cluster on virtual machines
  - Learned how Hadoop handles **DataNode failures**:
    - Temporary vs Permanent failure
    - Heartbeat mechanism & block replication
    - HDFS fault-tolerant architecture

‚úÖ **Day 16 ‚Äì Hadoop HA & HDFS Write Flow**
- Explored how Hadoop ensures fault tolerance and service continuity in enterprise environments.
- Topics covered:
  - **Secondary NameNode** ‚Äì Handles periodic merge of fsimage and edit logs
  - **Standby NameNode** ‚Äì Supports failover in HA setup
  - **Hadoop High Availability (HA)** ‚Äì Architecture using ZooKeeper and JournalNodes
  - **HDFS Write Process** ‚Äì Client to NameNode to DataNode pipeline.

‚úÖ **Day 17 ‚Äì GCP Hadoop Cluster, Linux & HDFS Commands**
- Deployed a working Hadoop cluster on **Google Cloud Platform (GCP)**.
- Topics covered:
  - **GCP Hadoop Cluster Setup** ‚Äì Created VMs, configured SSH and firewall rules
  - **Exploring Cluster Nodes** ‚Äì Verified services, web UI, and logs
  - **GCP Best Practices** ‚Äì Resource tagging, security, region zoning
  - **Linux Commands** ‚Äì File, process, and system navigation using CLI
  - **HDFS CLI** ‚Äì Upload, view, delete files and check block info in Hadoop
  
‚úÖ **Day 18 ‚Äì MapReduce & Distributed Processing**
- Learned the fundamentals of MapReduce and how it powers large-scale distributed data processing.
- Topics covered:
  - Map & Reduce functions
  - Key-value pair transformations
  - Workflow: Input ‚Üí Map ‚Üí Shuffle & Sort ‚Üí Reduce ‚Üí Output
  - Role of cluster in parallel execution
 
‚úÖ **Day 19 ‚Äì YARN Architecture & Process**
- Learned about YARN: the key layer in Hadoop for job scheduling and resource management.
- Topics covered:
  - YARN components: ResourceManager, NodeManager, ApplicationMaster, Containers
  - Analogy for simplified understanding
  - Full lifecycle of a job in YARN: from submission to completion

‚úÖ Day 20 ‚Äì Apache Spark Basics & Advantages

Started my journey into Apache Spark: a powerful engine for big data processing.

Topics covered:

What is Apache Spark and how it's different from Hadoop MapReduce

Key advantages of Spark: in-memory computation, speed, scalability, fault tolerance

Spark Ecosystem Overview: Spark Core, SQL, Streaming, MLlib, GraphX

Common Spark Interview Questions and real-world use cases

Understood Spark's relevance in modern data engineering pipelines
